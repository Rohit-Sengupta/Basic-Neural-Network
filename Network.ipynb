{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is just for all the necessary classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(logits):\n",
    "        exps = np.exp(logits - np.max(logits))\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "def to_one_hot(y, num_classes):\n",
    "        one_hot = np.zeros(num_classes)\n",
    "        one_hot[y] = 1\n",
    "        return one_hot\n",
    "\n",
    "# Exception if there is a mismatch\n",
    "class Mismatch(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "\n",
    "# Exception for bias isn't found\n",
    "class NoBias(Exception):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "\n",
    "class Neuron:\n",
    "    # Neuron Constructor\n",
    "    def __init__(self):\n",
    "        # Random until tuned by backpropagation\n",
    "        self.bias = np.random.randn()\n",
    "        self.output = 0\n",
    "        self.input = []\n",
    "\n",
    "\n",
    "    # Set  bias\n",
    "    def set_bias(self, bias):\n",
    "        self.bias = bias\n",
    "    \n",
    "    # Get  bias\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    # Layer Constructor\n",
    "    def __init__(self, size, input_size):\n",
    "        self.size = size\n",
    "        self.input_size = input_size\n",
    "        self.neurons = [Neuron() for _ in range(size)]\n",
    "        self.weights = np.random.randn(size, input_size)\n",
    "        self.input = None  # Placeholder for input data\n",
    "\n",
    "    # Function used to change the weights of the neurons in a layer\n",
    "    def change_weights(self, changes):\n",
    "        if changes.shape != self.weights.shape:\n",
    "            raise Mismatch(\"Weight changes shape does not match the weights shape\")\n",
    "        self.weights = changes\n",
    "\n",
    "    # Function used to change the biases of the neurons in a layer\n",
    "    def change_biases(self, changes):\n",
    "        if len(changes) != len(self.neurons):\n",
    "            raise Mismatch(\"Bias changes length does not match the number of neurons\")\n",
    "        for i in range(len(self.neurons)):\n",
    "            self.neurons[i].set_bias(changes[i])\n",
    "\n",
    "    # Gradient descent algorithm to apply backpropagation\n",
    "    def gradient_descent(self, errors, lr):\n",
    "        # Convert input to a NumPy array if it isn't already\n",
    "        inputs = np.array(self.input)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        dw = np.outer(errors, inputs)  # Shape: (num_neurons, num_inputs)\n",
    "        db = errors  # Shape: (num_neurons,)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        dw = np.clip(dw, -1, 1)\n",
    "        db = np.clip(db, -1, 1)\n",
    "        \n",
    "        # Change the weight towards the gradient descent\n",
    "        self.weights -= lr * dw\n",
    "        \n",
    "        # Change the bias towards the gradient descent\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            neuron.set_bias(neuron.get_bias() - lr * db[i])\n",
    "\n",
    "    # Give input to a layer\n",
    "    def give_input(self, input_data):\n",
    "        if len(input_data) != self.input_size:\n",
    "            raise Mismatch(\"Input size does not match the expected input size\")\n",
    "        self.input = input_data\n",
    "    # Use the formula output= (input*weight)+bias\n",
    "    # Function to calculate the outputs of the neurons\n",
    "    def calculate_outputs(self):\n",
    "        inputs = np.array(self.input)\n",
    "        weights = np.array(self.weights)\n",
    "        biases = np.array([neuron.get_bias() for neuron in self.neurons])\n",
    "        outputs = np.dot(weights, inputs) + biases\n",
    "        return outputs\n",
    "\n",
    "    # Function to get the outputs of the layer\n",
    "    def get_output(self):\n",
    "        return self.calculate_outputs().tolist()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class NN:\n",
    "    # Neural network constructor\n",
    "    def __init__(self, input_size, hidden_layers_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers_sizes = hidden_layers_sizes\n",
    "        self.output_size = output_size\n",
    "        #Input layer isn't an actual layer btw it's just a placeholder for inputs to go to hidden layer\n",
    "        #No calculations happen in the input layer\n",
    "                \n",
    "        # Initialize hidden layers\n",
    "        self.hidden_layers = []\n",
    "        for size in hidden_layers_sizes:\n",
    "            self.hidden_layers.append(Layer(size, input_size))\n",
    "            input_size = size\n",
    "        \n",
    "        # Initialize output layer\n",
    "        self.output_layer = Layer(output_size, input_size)\n",
    "\n",
    "    # Forward Propagation algorithm\n",
    "    # It's simply giving one layer's output as another layer's input\n",
    "    def forward_propagation(self, input_data):\n",
    "        input_O=input_data\n",
    "        temp = np.array(input_O).flatten()\n",
    "        for layer in self.hidden_layers:\n",
    "            layer.give_input(temp)\n",
    "            temp = layer.get_output()\n",
    "        hidden_O = temp\n",
    "        self.output_layer.give_input(hidden_O)\n",
    "        output_O = self.output_layer.get_output()\n",
    "        prediction=softmax(output_O)\n",
    "        return prediction\n",
    "\n",
    "    # Backward Propagation algorithm\n",
    "    def back_propagation(self, actual, input_data, lr):\n",
    "        output = self.forward_propagation(input_data)\n",
    "        actual=to_one_hot(actual,len(output))\n",
    "        \n",
    "        # Derivative of mean squared error formula\n",
    "        Error_O = 2 * (output - actual) / len(output)\n",
    "\n",
    "        Hidden_weights = self.output_layer.weights.T\n",
    "        self.output_layer.gradient_descent(Error_O, lr)\n",
    "\n",
    "        Error_H_end = np.dot(Hidden_weights, Error_O)\n",
    "        temp_E = Error_H_end\n",
    "\n",
    "        for layer in reversed(self.hidden_layers):\n",
    "            weights = layer.weights.T\n",
    "            layer.gradient_descent(temp_E, lr)\n",
    "            temp_E = np.dot(weights, temp_E)\n",
    "\n",
    "        Input_weights = self.hidden_layers[0].weights\n",
    " \n",
    "\n",
    "    # Training method\n",
    "    def train(self, training, answers, epoch, lr):\n",
    "        for e in range(epoch):\n",
    "            for input_data, expected_output in zip(training, answers):\n",
    "                self.back_propagation(expected_output, input_data, lr)\n",
    "            print(f\"Epoch {e+1}/{epoch} completed\")\n",
    "\n",
    "    # Testing method\n",
    "    def test(self, test_data, test_answers):\n",
    "        correct_predictions = 0\n",
    "        for input_data, expected_output in zip(test_data, test_answers):\n",
    "            output = self.forward_propagation(input_data)\n",
    "            predicted_class = np.argmax(output)  # Get the predicted class\n",
    "            if predicted_class == expected_output:\n",
    "                correct_predictions += 1\n",
    "        accuracy = correct_predictions / len(test_data)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Number of neurons in output layer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a neural network\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nn \u001b[38;5;241m=\u001b[39m NN(input_size\u001b[38;5;241m=\u001b[39minput_size, hidden_layers_sizes\u001b[38;5;241m=\u001b[39mhidden_size, output_size\u001b[38;5;241m=\u001b[39moutput_size)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example training data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m training_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NN' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_size = 576  # Number of features\n",
    "hidden_size = [64,32,16,8]  # Number of neurons in hidden layers\n",
    "output_size = 2  # Number of neurons in output layer\n",
    "\n",
    "# Create a neural network\n",
    "nn = NN(input_size=input_size, hidden_layers_sizes=hidden_size, output_size=output_size)\n",
    "\n",
    "# Example training data\n",
    "training_data = [\n",
    "\n",
    "]\n",
    "\n",
    "# Extend training data with random values\n",
    "for i in range(10000):\n",
    "\n",
    "    temp = []\n",
    "    for i in range(input_size):\n",
    "        temp.append(np.random.randn())\n",
    "    training_data.append(temp)\n",
    "\n",
    "# Example expected outputs\n",
    "answers = []\n",
    "\n",
    "# Extend answers with random binary values\n",
    "for i in range(10000):\n",
    "    temp = [np.random.choice([0, 1])]\n",
    "    print(temp)\n",
    "    answers.append(temp)\n",
    "\n",
    "# Train the network\n",
    "epochs = 3\n",
    "learning_rate = 0.0001\n",
    "nn.train(training_data, answers, epoch=epochs, lr=learning_rate)\n",
    "nn.test(training_data, answers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_X, test_y), (train_X, train_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((train_X.shape[0], 28*28)).astype('float32')\n",
    "test_X= test_X.reshape((test_X.shape[0], 28*28)).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X=train_X/255\n",
    "test_X=test_X/255\n",
    "test_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training\n",
      "0.12646666666666667\n",
      "=============Training===========\n",
      "Epoch 1/8 completed\n",
      "Epoch 2/8 completed\n",
      "Epoch 3/8 completed\n",
      "Epoch 4/8 completed\n",
      "Epoch 5/8 completed\n",
      "Epoch 6/8 completed\n",
      "Epoch 7/8 completed\n",
      "Epoch 8/8 completed\n",
      "Test after training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8362"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 28*28  # Number of features\n",
    "hidden_size = [128,64,36,18,10]  # Number of neurons in hidden layers\n",
    "output_size = 10\n",
    "epochs = 8\n",
    "learning_rate = 0.001\n",
    "nn = NN(input_size=input_size, hidden_layers_sizes=hidden_size, output_size=output_size)\n",
    "\n",
    "training_data=train_X\n",
    "answers=train_y\n",
    "testing_data=test_X\n",
    "test_answers=test_y\n",
    "print(\"Test before training\")\n",
    "\n",
    "accuracy=nn.test(testing_data, test_answers)\n",
    "print(accuracy)\n",
    "\n",
    "print(\"=============Training===========\")\n",
    "nn.train(training_data, answers, epoch=epochs, lr=learning_rate)\n",
    "\n",
    "print(\"Test after training\")\n",
    "nn.test(testing_data, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
